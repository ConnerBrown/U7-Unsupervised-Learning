{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 310](https://github.com/GonzagaCPSC310) Data Mining\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Association Rule Mining\n",
    "What are our learning objectives for this lesson?\n",
    "* Introduce association rule mining\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rule Mining\n",
    "* Tries to find any rule, not just those that predict a class label\n",
    "* Any attribute(s) becomes the label\n",
    "* Example of \"unsupervised learning\"\n",
    "\n",
    "Sometimes expressed as IF-THEN rules with conjunctive \"bodies\" and \"heads\"\n",
    "* IF $att_i = val_i \\wedge att_{k} = val_k \\wedge$ ... THEN $att_u = val_u \\wedge att_z = val_z \\wedge$ ...\n",
    "    * The left-hand side (LHS) is the rule body\n",
    "    * The right-hand side (RHS) is the rule head\n",
    "\n",
    "Similar to decision trees, but...\n",
    "* Head attributes don't have to be designated labels\n",
    "* Can have multiple attributes in the head\n",
    "\n",
    "Rules suggest a possible association\n",
    "* Between values of LHS attributes and values of RHS attributes\n",
    "* Rules do not suggest causality!\n",
    "\n",
    "Q: Brainstorm ways to perform rule mining ...\n",
    "* Brute force (i.e., try all combinations)\n",
    "* There are tricks to do better (we'll discuss some of these)\n",
    "\n",
    "Q: What are the issues/difficulties of rule mining?\n",
    "* Performance!\n",
    "    * Lots of combinations\n",
    "    * Even when using the performance \"tricks\"\n",
    "* Low \"signal\"... the \"interestingness\" of rules\n",
    "    * Rules may be very accurate but also rare\n",
    "    * e.g., body rarely occurs and/or head rarely occurs... think overfitting\n",
    "    * Recall independence assumption in Naive Bayes\n",
    "\n",
    "Rules likely **will not** be 100% accurate\n",
    "* Need metrics for measuring rule accuracy\n",
    "* As well as \"interestingness\"..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interestingness Measures\n",
    "Basics:\n",
    "* IF left THEN right\n",
    "    * $N_{left}$ = # of instances matching left\n",
    "    * $N_{right}$ = # of instances matching right\n",
    "    * $N_{both}$ = # of instances matching both left and right\n",
    "    * $N_{total}$ = # of instances in the dataset\n",
    "\n",
    "Confidence\n",
    "* Proportion of RHSs predicted by the rule that are correctly predicted\n",
    "$$\\frac{N_{both}}{N_{left}}$$\n",
    "* Similar in spirit to accuracy ($\\frac{TP+TN}{P+N}$)\n",
    "\n",
    "Support\n",
    "* Proportion of data set correctly predicted by the rule\n",
    "$$\\frac{N_{both}}{N_{total}}$$\n",
    "* One measure of \"interestingness\" (or \"rareness\")\n",
    "\n",
    "Completeness\n",
    "* Proportion of matching RHSs correctly predicted by the rule\n",
    "$$\\frac{N_{both}}{N_{right}}$$\n",
    "* Similar to estimate for $P(left|right)$\n",
    "\n",
    "We'll look at more later ...\n",
    "\n",
    "### Lab Task 1\n",
    "Calculate metrics for some rules of the iPhone Purchases (Fake) dataset:\n",
    "\n",
    "|standing |job_status |credit_rating |buys_iphone|\n",
    "|-|-|-|-|\n",
    "|1 |3 |fair |no|\n",
    "|1 |3 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|2 |1 |fair |yes|\n",
    "|2 |1 |excellent |no|\n",
    "|2 |1 |excellent |yes|\n",
    "|1 |2 |fair |no|\n",
    "|1 |1 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|1 |2 |excellent |yes|\n",
    "|2 |2 |excellent |yes|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "\n",
    "### Lab Task 2\n",
    "What is the upper bound on the number of RHSs (step 1) assuming we only know\n",
    "the domain values of each attribute ... 83 = 9 + 30 + 44\n",
    "\n",
    "## J-Measure\n",
    "Basic Idea\n",
    "* Find the best $N$ rules from $D$ ... $N$ is a parameter\n",
    "* Use $J$-Measure as a quality metric for rules\n",
    "* Prune search space for rules (search \"strategy\")\n",
    "\n",
    "Parameters\n",
    "* Number of rules $N$ (often small ... 10 to 20)\n",
    "* Number of LHS terms to consider (max number) ... \"rule order\"\n",
    "* Number of RHS terms to consider (e.g., 1)\n",
    "\n",
    "The Basic Search Strategy Assume RHSs have 1 term\n",
    "1. Generate all RHSs\n",
    "2. Generate all order 1 rules\n",
    "3. Calculate $J$-value for each rule\n",
    "4. Keep the top $N$ rules\n",
    "5. \"Specialize\" best $N$ rules by increasing order, again keeping only the top $N$ rules (i.e., from the original $N$ + specialized)\n",
    "\n",
    "Referred to as a \"beam\" search\n",
    "* Beam width is $N$\n",
    "* Not guaranteed to find best rules\n",
    "* Can still be expensive\n",
    "\n",
    "To reduce the search space further, can exploit $J_{max}$\n",
    "* Can test if specializing a rule can improve $J$-Value\n",
    "* If not, skip it\n",
    "\n",
    "## Measuring the Information Content of Rules\n",
    "* Given a rule:\n",
    "\n",
    "Rule1: IF left THEN right  \n",
    "... or just ...  \n",
    "Rule1: IF L THEN R\n",
    "\n",
    "* The $J$-Value is: \n",
    "$$J(Rule1) = P(L) \\times j(Rule1)$$\n",
    "* Where $j$ is the \"cross entropy\" (measured in bits of information)\n",
    "$$j(Rule1) = P(R|L) \\times log_{2}\\frac{P(R|L)}{P(R)} + (1 - P(R|L)) \\times log_{2}\\frac{1 - P(R|L)}{1 - P(R)}$$\n",
    "    * Roughly the amount of information (average number of bits) needed to distinguish between $P(R|L)$ and $1 - P(R|L)$\n",
    "    * The more information needed, the more dissimilar the two are\n",
    "* The P's are estimated as:\n",
    "$$P(R) = \\frac{N_{right}}{N_{total}}$$\n",
    "$$P(L) = \\frac{N_{left}}{N_{total}}$$\n",
    "$$P(R|L) = \\frac{N_{both}}{N_{left}}$$ ... confidence!\n",
    "\n",
    "## \"Specializing\" Rules\n",
    "* Adding terms to the LHS (increasing the order)\n",
    "* $J$-Value of a rule obtained through specializing a given rule is bounded by:\n",
    "$$J_{max} = P(L) \\times max\\{P(R|L) \\times log_{2}\\frac{1}{P(R)}, (1 - P(R|L)) \\times log_{2}\\frac{1}{1 - P(R)}\\}$$\n",
    "\n",
    "* So, e.g., if a rule $Rule_i$ already has $J(Rule_i) = J_{max}(Rule_i)$, then no reason to further specialize (pruning)\n",
    "* Or, if $J_{max}(Rule_i)$ puts it out of the new top $N$, no reason to specialize further (also pruning)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
